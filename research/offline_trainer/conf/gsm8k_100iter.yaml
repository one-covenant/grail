# Full-scale GSM8K GRPO training (100 iterations) - OPTIMIZED
defaults:
  - offline_grpo

# Model
model:
  train_id: "Qwen/Qwen2.5-1.5B-Instruct"
  ref_id: "Qwen/Qwen2.5-1.5B-Instruct"

# Training (Optimized GRPO for math reasoning)
train:
  iterations: 100
  batch_size: 4  # Reduced for memory optimization with bf16
  lr: 2.0e-6  # Better convergence for math (CRITICAL FIX: was 1.0e6!)
  grad_accum_steps: 32  # Effective batch: 128 (faster iteration)
  grad_clip: 1.0  # Tighter clipping for stability
  warmup_steps: 50  # 5% warmup for better convergence
  max_length: 1024
  group_adv_sum_tol: 0.01

  # Advantage clipping
  adv_clip_percentile: 98.0  # More aggressive outlier removal

  # KL divergence (disabled for pure policy gradient)
  kl_coef: 0.0  # Disabled - pure on-policy GRPO
  entropy_coef: 0.001  # Exploration bonus for math problems
  adaptive_kl: false  # Disabled with kl_coef=0
  kl_target: 0.04
  kl_adapt_rate: 1.5
  kl_min: 0.001
  kl_max: 0.2

  # Importance sampling and PPO clipping (tight trust region for math)
  use_is: true  # Enable importance sampling
  ppo_clip_eps: 0.2  # TRAINER_PPO_CLIP_EPS
  ppo_clip_eps_upper: 0.28  # TRAINER_PPO_CLIP_EPS_UPPER
  is_ratio_max: 2.5  # GRAIL_TRAINER_IS_RATIO_MAX
  logratio_clamp: 0.92  # GRAIL_TRAINER_LOGRATIO_CLAMP

# Data
data:
  environment: "gsm8k"
  problems_per_iteration: 32  # Diverse training signal
  rollouts_per_problem: 16  # Good advantage variance
  generation_batch_size: 4  # Process 4 problems at a time (64 rollouts total per batch)
  train_seed_start: 0
  num_train_seeds: 3200  # Large pool (100 iter Ã— 32 prob = 3200 unique calls)

# Evaluation
eval:
  enabled: true
  interval: 10  # Every 10 iterations
  num_ids: 200  # Comprehensive coverage
  replicates: 16  # Robust pass@k estimates
  id_seed_start: 5000

# Checkpointing
checkpoint:
  save_interval: 10
  keep_last_k: 5

# Logging
logging:
  wandb:
    enabled: true
    project: "grail-offline"
    entity: "tplr"
    tags: ["gsm8k", "grpo", "100-iter", "1.5b", "optimized"]
