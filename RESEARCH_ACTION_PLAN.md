# Actionable Research Plan: From Observation to Publication

## ðŸŽ¯ Immediate Next Steps (Week 1-2)

### Step 1: Validate the Core Observation

**Goal**: Systematically measure and characterize weight update sparsity across different conditions.

#### Experiment 1A: Sparsity vs. Training Stage
```python
# experiments/measure_sparsity.py

import torch
from collections import defaultdict

class SparsityTracker:
    """Track weight update sparsity during training."""

    def __init__(self, model, threshold=0.0):
        self.threshold = threshold
        self.prev_state = {
            name: param.data.clone()
            for name, param in model.named_parameters()
        }
        self.history = defaultdict(list)

    def step(self, step_num):
        """Measure sparsity after optimizer step."""

        # Global sparsity
        total_params = 0
        changed_params = 0

        # Per-layer sparsity
        layer_stats = {}

        for name, param in model.named_parameters():
            prev = self.prev_state[name]
            delta = param.data - prev

            # Count changes
            mask = delta.abs() > self.threshold
            changed = mask.float().sum().item()
            total = delta.numel()

            total_params += total
            changed_params += changed

            # Track per-layer
            layer_stats[name] = {
                "sparsity": 1 - (changed / total),
                "max_change": delta.abs().max().item(),
                "mean_change": delta.abs().mean().item(),
                "std_change": delta.abs().std().item(),
            }

            # Update prev state
            self.prev_state[name] = param.data.clone()

        # Global stats
        global_sparsity = 1 - (changed_params / total_params)

        # Record
        self.history["global_sparsity"].append(global_sparsity)
        self.history["changed_params"].append(changed_params)
        self.history["step"].append(step_num)

        for name, stats in layer_stats.items():
            for metric, value in stats.items():
                self.history[f"{name}/{metric}"].append(value)

        return {
            "global_sparsity": global_sparsity,
            "changed_params": changed_params,
            "total_params": total_params,
            "layer_stats": layer_stats,
        }
```

**Run Matrix**:
```python
# Configuration matrix for systematic study
configs = [
    # Vary model size
    {"model": "gpt2", "task": "rlhf"},
    {"model": "gpt2-medium", "task": "rlhf"},
    {"model": "gpt2-large", "task": "rlhf"},

    # Vary learning rate
    {"model": "gpt2", "task": "rlhf", "lr": 1e-6},
    {"model": "gpt2", "task": "rlhf", "lr": 1e-5},
    {"model": "gpt2", "task": "rlhf", "lr": 1e-4},

    # Vary optimizer
    {"model": "gpt2", "task": "rlhf", "optimizer": "adam"},
    {"model": "gpt2", "task": "rlhf", "optimizer": "sgd"},
    {"model": "gpt2", "task": "rlhf", "optimizer": "adafactor"},

    # Compare RL vs. supervised
    {"model": "gpt2", "task": "rlhf"},
    {"model": "gpt2", "task": "sft"},  # Supervised fine-tuning
    {"model": "gpt2", "task": "pretrain"},  # From scratch

    # Vary reward signal
    {"model": "gpt2", "task": "rlhf", "reward": "helpful"},
    {"model": "gpt2", "task": "rlhf", "reward": "harmless"},
    {"model": "gpt2", "task": "rlhf", "reward": "truthful"},
]

for config in configs:
    run_experiment(config)
    analyze_sparsity_patterns()
```

**Expected Outcomes**:
1. **Sparsity is consistent**: 98-99% across models/tasks
2. **Sparsity varies by layer**: Attention < FFN < LayerNorm
3. **Sparsity increases over time**: More sparse as training progresses
4. **RL is sparser than SFT**: Policy gradients â†’ localized updates

**Deliverable**: Figure for paper showing sparsity across conditions.

---

### Step 2: Characterize Which Weights Change

#### Experiment 1B: Spatial Patterns
```python
def analyze_spatial_patterns(tracker):
    """Find which weights consistently change."""

    # Aggregate: which parameters change most often?
    change_frequency = defaultdict(int)

    for step in range(num_steps):
        delta = tracker.get_delta(step)

        for name, param_delta in delta.items():
            changed_indices = (param_delta.abs() > threshold).nonzero()

            for idx in changed_indices:
                change_frequency[(name, tuple(idx))] += 1

    # Find "hot" parameters (change >50% of time)
    hot_params = {
        (name, idx): freq / num_steps
        for (name, idx), freq in change_frequency.items()
        if freq > 0.5 * num_steps
    }

    return hot_params
```

**Research Questions**:
- Q1: Are there "critical" weights that always change?
- Q2: Are changes random or structured (e.g., attention heads)?
- Q3: Can we predict which weights will change next?

**Deliverable**: Heatmap visualization of weight change frequency.

---

### Step 3: Measure Effective Rank

#### Experiment 1C: Gradient Rank Analysis
```python
def measure_gradient_rank(model, dataloader, k=100):
    """Measure effective rank of gradient updates."""

    # Collect gradients over multiple batches
    grads = []

    for batch in dataloader[:k]:
        model.zero_grad()
        loss = compute_loss(model, batch)
        loss.backward()

        # Flatten gradients
        grad_flat = torch.cat([
            p.grad.flatten()
            for p in model.parameters()
            if p.grad is not None
        ])
        grads.append(grad_flat)

    # Stack into matrix [k, num_params]
    grad_matrix = torch.stack(grads)

    # Compute SVD
    U, S, V = torch.svd(grad_matrix)

    # Effective rank: number of singular values > threshold
    effective_rank = (S > 0.01 * S[0]).sum().item()

    return {
        "full_rank": len(S),
        "effective_rank": effective_rank,
        "rank_ratio": effective_rank / len(S),
        "singular_values": S.cpu().numpy(),
    }
```

**Expected Result**: Effective rank << full rank (e.g., 1000 vs. 7B)

**Implication**: Gradients lie in low-dimensional subspace â†’ sparse updates are theoretically optimal.

---

## ðŸ“ Paper 1: Empirical Analysis (Workshop â†’ Conference)

### Title: "The Surprising Sparsity of Policy Gradient Updates"

### Abstract (Draft)
```
We present a systematic study of weight update sparsity in reinforcement
learning fine-tuning. Across models ranging from 124M to 7B parameters,
we find that only 1-5% of weights change significantly per optimization
step, despite using dense gradient computation. We characterize this
phenomenon across model architectures, optimizers, and tasks, and show
that sparsity is:

1. Consistent across model sizes (98-99% sparse)
2. Layer-dependent (attention < FFN < embeddings)
3. Task-dependent (RL > SFT > pretraining)
4. Predictor of convergence (higher sparsity â†’ better performance)

We provide a geometric interpretation via Neural Tangent Kernel analysis,
showing that policy gradients are inherently low-rank due to the structure
of advantage estimation. Our findings suggest opportunities for
communication-efficient distributed RL and sparse-first optimization.
```

### Outline
1. **Introduction**
   - Motivation: Communication bottleneck in distributed RL
   - Observation: Weight updates are 98% sparse
   - Contributions: Systematic characterization + geometric theory

2. **Methodology**
   - Models: GPT-2 (124M, 355M, 774M), Pythia (1.4B, 2.8B)
   - Tasks: RLHF (helpful, harmless), SFT, pretraining
   - Metrics: Sparsity, effective rank, spatial patterns

3. **Results**
   - Figure 1: Sparsity vs. training step (all models)
   - Figure 2: Layer-wise sparsity breakdown
   - Figure 3: Gradient rank vs. model size
   - Table 1: Sparsity across tasks (RL vs. SFT vs. pretrain)

4. **Theoretical Analysis**
   - NTK perspective: Why are RL gradients low-rank?
   - Proposition: Sparse advantage â†’ sparse gradients
   - Empirical validation: Advantage sharpness vs. sparsity

5. **Applications**
   - Communication-efficient federated RL
   - Sparse-first optimization algorithms
   - Model merging via delta composition

6. **Related Work**
   - Gradient compression (PowerSGD, QSGD, Top-K)
   - Sparse training (lottery tickets, magnitude pruning)
   - Low-rank adaptation (LoRA, QLoRA)

7. **Conclusion**
   - Sparsity is fundamental to RL fine-tuning
   - Opportunities for systems and theory
   - Future work: Exploit sparsity end-to-end

### Target Venues (Ordered by Priority)
1. **NeurIPS 2025 Workshop** (WANT: ML + Networks) â€” **Deadline: Sept 2025**
2. **ICLR 2026** (Main conference) â€” **Deadline: Oct 2025**
3. **ICML 2026** (Main conference) â€” **Deadline: Feb 2026**

---

## ðŸ“Š Paper 2: Sparse Optimization Algorithm (Main Conference)

### Title: "Adaptive Sparse Policy Gradients: Communication-Efficient RL at Scale"

### Core Contribution: Algorithm + Theory + Experiments

#### Algorithm: ASPG (Adaptive Sparse Policy Gradients)
```python
class ASPG(Optimizer):
    """Adaptive Sparse Policy Gradient optimizer.

    Key idea: Only compute/communicate gradients for top-k% parameters
    by estimated importance.
    """

    def __init__(self, params, lr=1e-5, sparsity=0.01, warmup_steps=100):
        self.lr = lr
        self.sparsity = sparsity
        self.warmup_steps = warmup_steps

        # Track importance scores (EMA of gradient magnitude)
        self.importance = {
            id(p): torch.zeros_like(p)
            for p in params
        }

    def step(self, closure=None):
        # Phase 1: Compute full gradients (for importance estimation)
        loss = closure()

        # Phase 2: Update importance scores (EMA)
        for p in self.param_groups[0]["params"]:
            if p.grad is None:
                continue

            importance = self.importance[id(p)]
            importance.mul_(0.9).add_(p.grad.abs(), alpha=0.1)

        # Phase 3: Select top-k% parameters to actually update
        all_importance = torch.cat([
            self.importance[id(p)].flatten()
            for p in self.param_groups[0]["params"]
        ])

        k = int(self.sparsity * len(all_importance))
        threshold = torch.topk(all_importance, k).values[-1]

        # Phase 4: Sparse update (only top-k)
        for p in self.param_groups[0]["params"]:
            if p.grad is None:
                continue

            # Mask: only update important parameters
            mask = self.importance[id(p)] >= threshold

            # Sparse update
            p.data[mask] -= self.lr * p.grad[mask]

            # Zero out other gradients (don't accumulate)
            p.grad[~mask] = 0
```

#### Theoretical Guarantee
**Theorem 1** (Convergence under sparse updates):

Under standard assumptions (L-smoothness, bounded variance), ASPG converges at rate:

```
E[||âˆ‡f(x_t)||Â²] â‰¤ O(1/T) + O(sparsity_error)

where sparsity_error â‰¤ Îµ if top-k selection captures (1-Îµ) of gradient mass.
```

**Proof sketch**: Error feedback accumulates dropped gradients, ensuring unbiasedness.

#### Experimental Validation

**Setup**:
- Models: GPT-2 (124M, 355M), Pythia (1.4B)
- Tasks: RLHF (Anthropic HH), summarization (TL;DR)
- Baselines: Dense Adam, LoRA, Prefix tuning

**Metrics**:
1. **Performance**: Reward model score (vs. SFT baseline)
2. **Efficiency**: Total FLOPs, wall-clock time
3. **Communication**: Total bytes sent (distributed setting)
4. **Sparsity**: Actual % parameters updated

**Results** (Projected):
- âœ… ASPG matches dense Adam in final performance
- âœ… 10x fewer FLOPs (sparse gradient computation)
- âœ… 100x less communication (only send sparse updates)
- âœ… Automatically adapts sparsity (no hyperparameter tuning)

### Target Venues
1. **ICLR 2026** (Spotlight/Oral goal) â€” **Deadline: Oct 2025**
2. **NeurIPS 2025** (if ready sooner) â€” **Deadline: May 2025**

---

## ðŸ› ï¸ Implementation Roadmap

### Week 1-2: Measurement Infrastructure
- [ ] Implement `SparsityTracker` class
- [ ] Run experiments across 10+ configurations
- [ ] Generate plots and tables for Paper 1
- [ ] **Deliverable**: `experiments/sparsity_analysis.py`

### Week 3-4: Theoretical Analysis
- [ ] Implement gradient rank measurement
- [ ] Derive NTK-based sparsity bound
- [ ] Validate with synthetic examples
- [ ] **Deliverable**: `analysis/gradient_rank.py`

### Week 5-6: Algorithm Development
- [ ] Implement ASPG optimizer
- [ ] Benchmark against baselines
- [ ] Tune hyperparameters (sparsity, warmup)
- [ ] **Deliverable**: `algorithms/aspg.py`

### Week 7-8: Large-Scale Experiments
- [ ] Run on 1.4B+ models (requires GPU cluster)
- [ ] Distributed setting (multi-node)
- [ ] Ablation studies (sparsity levels)
- [ ] **Deliverable**: Results for Paper 2

### Week 9-10: Writing
- [ ] Draft Paper 1 (workshop)
- [ ] Submit to NeurIPS WANT workshop
- [ ] Incorporate feedback
- [ ] **Deliverable**: Workshop paper submission

### Week 11-16: Extension to Conference
- [ ] Expand experiments (more models/tasks)
- [ ] Strengthen theory (tighter bounds)
- [ ] Draft Paper 2 (main conference)
- [ ] **Deliverable**: ICLR 2026 submission

---

## ðŸ“ˆ Success Metrics

### Paper 1 (Workshop)
- âœ… Acceptance to NeurIPS workshop
- âœ… Reproducible results (open-source code)
- âœ… Citation count >10 within 6 months

### Paper 2 (Conference)
- ðŸŽ¯ Acceptance to ICLR/NeurIPS (top-4 venue)
- ðŸŽ¯ Spotlight or Oral (top 5% of submissions)
- ðŸŽ¯ Integration into popular libraries (HuggingFace, PyTorch)

---

## ðŸŽ“ Backup Plans (If Things Go Wrong)

### Scenario 1: Sparsity is Not Universal
**Problem**: Sparsity only holds for specific settings (e.g., small LR)

**Pivot**: Characterize when sparsity emerges
- "Conditions for Sparse Policy Gradients"
- Focus on phase transitions (sparse â†’ dense)

### Scenario 2: Algorithm Doesn't Work
**Problem**: ASPG performs worse than dense baseline

**Pivot**: Diagnostic paper
- "Why Sparse Optimization Fails for RL"
- Identify failure modes, propose solutions

### Scenario 3: Theory is Too Weak
**Problem**: Cannot prove convergence guarantees

**Pivot**: Empirical systems paper
- "SparseComm: A System for Communication-Efficient Federated RL"
- Focus on engineering, measure real-world impact

---

## ðŸ’¡ Key Research Insights to Validate

### Hypothesis 1: Sparsity â‰  Compression
**Claim**: Sparsity is fundamental, not just a compression trick.

**Test**: Does enforcing sparsity (e.g., top-k pruning) hurt or help?
- If it helps â†’ sparsity is beneficial (regularization)
- If it hurts â†’ sparsity is just artifact (no value)

### Hypothesis 2: Sparsity Predicts Performance
**Claim**: Models with higher sparsity converge faster.

**Test**: Correlation between sparsity and final reward.
- If positive â†’ sparsity is good
- If negative â†’ sparsity is bad

### Hypothesis 3: Sparsity is Layer-Specific
**Claim**: Different layers have different sparsity patterns.

**Test**: Train layer-specific sparsity thresholds.
- If performance improves â†’ heterogeneous sparsity matters

---

## ðŸ“š Literature Review (Must Read)

### Sparse Training
1. **"The Lottery Ticket Hypothesis"** (Frankle & Carbin, ICLR 2019)
   - Sparse subnetworks can match dense performance
   - Connection to our work: Are sparse updates finding lottery tickets?

2. **"Rigging the Lottery"** (Frankle et al., ICML 2020)
   - Structured pruning for efficiency
   - Connection: Can we predict sparsity structure?

### Gradient Compression
3. **"Deep Gradient Compression"** (Lin et al., ICLR 2018)
   - Top-k gradient selection for communication
   - Connection: Our work shows top-k is natural for RL

4. **"PowerSGD"** (Vogels et al., NeurIPS 2019)
   - Low-rank gradient compression
   - Connection: We show RL gradients are intrinsically low-rank

### Low-Rank Adaptation
5. **"LoRA"** (Hu et al., ICLR 2022)
   - Low-rank weight updates for fine-tuning
   - Connection: Our sparse updates are complementary to LoRA

6. **"QLoRA"** (Dettmers et al., NeurIPS 2023)
   - Quantized low-rank adaptation
   - Connection: Can combine with sparse updates?

### Federated Learning
7. **"FedAvg"** (McMahan et al., AISTATS 2017)
   - Baseline for federated optimization
   - Connection: Our sparse protocol is drop-in replacement

8. **"FedProx"** (Li et al., MLSys 2020)
   - Proximal updates for heterogeneous data
   - Connection: Does sparsity help with non-IID?

---

## ðŸŽ¯ Concrete Milestones (6-Month Plan)

### Month 1: Data Collection
- âœ… Run sparsity measurement experiments
- âœ… Collect data across 10+ configurations
- âœ… Visualize results (plots, tables)

### Month 2: Analysis
- âœ… Analyze spatial/temporal patterns
- âœ… Measure gradient rank
- âœ… Write theory section (NTK analysis)

### Month 3: Algorithm
- âœ… Implement ASPG optimizer
- âœ… Run small-scale experiments (GPT-2)
- âœ… Compare to baselines

### Month 4: Scaling
- âœ… Run large-scale experiments (1.4B+ models)
- âœ… Distributed setting (8+ nodes)
- âœ… Collect metrics (performance, efficiency)

### Month 5: Writing
- âœ… Draft workshop paper (RD1: Empirical)
- âœ… Submit to NeurIPS WANT
- âœ… Start draft of main conference paper (RD2: Algorithm)

### Month 6: Iteration
- âœ… Incorporate workshop feedback
- âœ… Extend experiments for main conference
- âœ… Submit to ICLR 2026

**Estimated Time to First Publication**: 5-6 months (workshop), 9-12 months (main conference)

---

## ðŸš€ Getting Started Today

### Immediate Actions (Next 24 Hours)

1. **Set up experiment tracking**:
   ```bash
   pip install wandb  # For tracking experiments
   wandb login
   ```

2. **Implement SparsityTracker**:
   - Copy code from "Step 1" above
   - Integrate into existing training loop
   - Run first experiment overnight

3. **Create results directory**:
   ```bash
   mkdir -p experiments/sparsity_analysis/{data,plots,logs}
   ```

4. **Run first experiment**:
   ```python
   # experiments/run_sparsity_experiment.py
   tracker = SparsityTracker(model, threshold=0.0)

   for step, batch in enumerate(dataloader):
       # Train
       loss = train_step(model, batch)
       optimizer.step()

       # Measure
       stats = tracker.step(step)

       # Log
       wandb.log(stats, step=step)
   ```

5. **Visualize results**:
   ```python
   # experiments/plot_sparsity.py
   import matplotlib.pyplot as plt

   plt.plot(tracker.history["step"], tracker.history["global_sparsity"])
   plt.xlabel("Training Step")
   plt.ylabel("Sparsity (%)")
   plt.title("Weight Update Sparsity During RL Fine-Tuning")
   plt.savefig("experiments/sparsity_analysis/plots/sparsity_vs_step.png")
   ```

**Expected Output**: Plot showing 98-99% sparsity within first 100 steps.

---

## ðŸ“§ Collaboration & Feedback

### Internal Validation
- Share results with team weekly
- Iterate on experiment design based on findings

### External Feedback
- Post preprint on arXiv
- Share on Twitter/Reddit (ML community)
- Present at lab meetings / reading groups

### Reproducibility
- Open-source all code (GitHub)
- Provide Docker environment
- Document hyperparameters

---

**Ready to start?** Begin with "Immediate Actions" and work through Month 1 milestones.

**Questions?** Refer back to `DELTA_PROTOCOL_DESIGN.md` for system design and `RESEARCH_DIRECTIONS.md` for broader context.

**Good luck! ðŸš€**
