"""GRAIL cryptographic proof validator.

Verifies that rollout tokens were generated by the claimed model using
cryptographic commitment and sketch-based hidden state verification.

Supports both legacy (s_vals) and MRS (Magnitude-Rank Sketch) proofs.
"""

from __future__ import annotations

import logging

import torch

from ...protocol.crypto import dot_mod_q, indices_from_root, r_vec_from_randomness
from ...protocol.signatures import verify_commit_signature
from ...shared.constants import (
    CHALLENGE_K,
    LAYER_INDEX,
    PRIME_Q,
    TOLERANCE,
)
from ...shared.hf_compat import resolve_vocab_size
from ..base import Validator
from ..context import ValidationContext

logger = logging.getLogger(__name__)


class GRAILProofValidator(Validator):
    """Verifies GRAIL cryptographic proof of model identity.

    This validator:
    1. Checks commit signature binding (tokens, s_vals, model, layer, randomness)
    2. Runs model inference to get hidden states
    3. Verifies sketch values match at challenged indices
    4. Caches logits for downstream validators (termination check)
    """

    check_name = "proof_valid"

    def validate(self, ctx: ValidationContext) -> bool:
        """Verify GRAIL proof and cache logits."""
        # Extract inputs
        try:
            tokens = ctx.commit["tokens"]
            s_vals = ctx.commit["s_vals"]
        except KeyError:
            logger.debug("Missing tokens or s_vals in commit")
            ctx.checks[self.check_name] = False
            return False

        # Validate structure
        if not isinstance(s_vals, list) or len(tokens) != len(s_vals):
            logger.debug(f"Invalid s_vals: len(tokens)={len(tokens)}, len(s_vals)={len(s_vals)}")
            ctx.checks[self.check_name] = False
            return False

        # Minimum sequence length check
        seq_len = len(tokens)
        if seq_len < CHALLENGE_K:
            logger.debug(f"Sequence too short: {seq_len} < {CHALLENGE_K}")
            ctx.checks[self.check_name] = False
            return False

        # Verify commit signature binding
        if not verify_commit_signature(ctx.commit, ctx.prover_address):
            logger.debug("Commit signature verification failed")
            ctx.checks[self.check_name] = False
            return False

        # Verify model/layer binding
        model_info = ctx.commit.get("model", {})
        expected_model = ctx.model.name_or_path
        if model_info.get("name") != expected_model:
            logger.debug(f"Model mismatch: expected {expected_model}, got {model_info.get('name')}")
            ctx.checks[self.check_name] = False
            return False

        try:
            layer_claim = int(model_info.get("layer_index"))
        except (TypeError, ValueError):
            logger.debug("Invalid layer_index in commit")
            ctx.checks[self.check_name] = False
            return False

        if layer_claim != LAYER_INDEX:
            logger.debug(f"Layer mismatch: expected {LAYER_INDEX}, got {layer_claim}")
            ctx.checks[self.check_name] = False
            return False

        # Derive sketch vector from beacon randomness
        beacon = ctx.commit.get("beacon", {})
        if not beacon or "randomness" not in beacon:
            logger.debug("Missing beacon randomness")
            ctx.checks[self.check_name] = False
            return False

        r_vec = r_vec_from_randomness(beacon["randomness"], ctx.model.config.hidden_size)

        # Derive challenge indices deterministically
        idxs = indices_from_root(tokens, ctx.challenge_randomness, seq_len, CHALLENGE_K)

        # Run model inference
        full_ids = torch.tensor(tokens, dtype=torch.long, device=ctx.device).unsqueeze(0)
        try:
            with torch.inference_mode():
                outs = ctx.model(full_ids, output_hidden_states=True)
        except RuntimeError as e:
            logger.error(f"Model inference failed: {e}")
            vocab_size = resolve_vocab_size(ctx.model.config)
            logger.error(f"Vocab={vocab_size}, tokens range=[{min(tokens)}, {max(tokens)}]")
            ctx.checks[self.check_name] = False
            return False

        h_layer = outs.hidden_states[LAYER_INDEX][0]

        # Cache logits for termination validator
        if outs.logits.size(1) >= 2:
            ctx.cached_logits = outs.logits[0, -2, :].detach().to("cpu")

        # Verify sketches at challenged indices
        for i in idxs:
            if i >= len(s_vals):
                logger.debug(f"Index {i} out of bounds for s_vals length {len(s_vals)}")
                ctx.checks[self.check_name] = False
                return False

            local = dot_mod_q(h_layer[i], r_vec)
            committed = s_vals[i]
            diff = abs(local - committed)
            mod_diff = min(diff, PRIME_Q - diff)  # Modular distance

            if mod_diff > TOLERANCE:
                logger.debug(
                    f"Sketch mismatch at {i}: local={local}, committed={committed}, diff={mod_diff}"
                )
                ctx.checks[self.check_name] = False
                return False

        logger.debug("GRAIL proof verification successful")
        ctx.checks[self.check_name] = True
        return True
